This file looks at quasi-extinction probability with heavy vs. normal tails.

**I'm currently adapting this script to bring in the parameter values from the model fitting.**

Bring in the fitted data:

```{r}
source("5-shape-data.R")
```

Now write a function that crawls through `gomp_hat_base` row by row and repeatedly draws all needed parameter values stochastically and feeds these into a forward projection of the Gompertz model.

Do downwards heavy tails really matter for extinction risk?

Two ways to go about this: repeatedly bootstrap from the observed process deviations or draw the deviations from a t distribution with given sigma and nu. The problem with the later is that the deviations won't be appropriately downwardly skewed. But the problem with the former is that I'm not sure this gets what we want out of the exercise. The assumed normal distributions might not in fact be normal. The model might just have fit poorly. Therefore, I'm going to start with the first option.

<!--
I'll get those deviations the same way I do for the spark line plots. The idea is this: for a given population, draw lambda and draw b stochastically. Now, find what the residuals would have been for those

```{r}
get_gomp_res <- function(id) {
  p <- subset(gomp_hat_base, main_id == id)
  pop <- subset(gpdd, main_id == id_show)$population_untransformed
  res <- rep(NA, length(pop))
  for(i in 2:length(pop)) {
    res[i] <- log(pop)[i] - (p$lambda_50 + p$b_50 * log(pop[i-1]))
  }
  return(list(res = res, l = l, u = u))
}
```


First, we'll figure out approximately what skewness values to use to get ballpark 85% downward deviations. This is to approximately match the skewness we see in the GPDD: (TODO come back to this skewness issue)

```{r}
library("skewt")
perc_downs <- function(df, gamma) {
  N <- 1e6
  proc_error <- rskt(N, df = df, gamma = gamma)
  # hist(proc_error)
  l <- qnorm(0.0001, 0, sd = 1)
  u <- qnorm(0.9999, 0, sd = 1)
  downs <- sum(proc_error < l)
  ups <- sum(proc_error > u)
  downs / (downs + ups)
}
perc_downs(df = 2, gamma = 0.75)
perc_downs(df = 3, gamma = 0.75)
```

So, 0.75 gets us in the ballpark of 85% percent downs.
-->

We'll write a little C++ function to simulate from a Gompertz model. We'll do this in C++ just to make everything fast after this.

```{r}
library("Rcpp")
cppFunction("
  NumericVector gompertz(double n, double lambda, double b,
    NumericVector proc_error, double y1) {
    NumericVector y(n);
    y(0) = y1;
    for (int i = 1; i < n; ++i) {
      y(i) = lambda + b * y(i - 1) + proc_error(i - 1);
    }
    return y;
}")
```

And we'll write a function to simulate a time series, check how many generations go below some threshold, and return the data in a form we can use. 

For each, we'll be drawing all parameter values from the posterior and starting at the state the time series ended. Then we'll continue on for some given number of generations.

<!--
I've picked parameter values to be approximately at the mean values in the GPDD (I'll check these more precisely). I'm running the simulation, by default, for 200 generations and discarding the first 50 as burn in.
-->

```{r}
# library("skewt")
sim_gomp_extinction <- function(abundance_ts, lambda = 1, b = 0.5, N = 100L,
  df = 3, thresh = 3, sigma = 0.2, id = 1, bootstrap_residuals = FALSE, 
  model = "heavy") {
  
  if (is.null(df)) df <- NA
  
  log_abundance_ts <- log(abundance_ts)
  y1 <- log_abundance_ts[length(abundance_ts)]
  
  if (bootstrap_residuals) {
    res <- log_abundance_ts[-1] - (lambda + b * log_abundance_ts[-length(abundance_ts)])
    proc_error <- sample(res, size = N, replace = TRUE) # bootstrap the process residuals
  } else {
    if (model == "heavy") {
      # proc_error <- rskt(N, df = df, gamma = gamma) / (1 / sigma)
      proc_error <- rt(N, df = df) * sigma
    } else {
      proc_error <- rnorm(N, sd = sigma)
    }
  }
  
  y <- gompertz(N, lambda, b, proc_error, y1)
  
    y_return <- c(log_abundance_ts, y)
    generation <- c(seq(-length(abundance_ts) + 1, 0), seq_along(y))
    data.frame(b = b, N = N,
      df = df, sigma = sigma, thresh = thresh, lambda = lambda,
      n_ext = sum(exp(y) < thresh), mean_log_abund = mean(y),
      dat = y_return, generation = generation, main_id = id)
}
```

OK, let's try applying this to some sample populations:

```{r}
ids <- c(6528, 10127, 20579)
library("rstan")
gpdd <- readRDS("gpdd-clean.rds") # to get the last abundance
samples <- 2000L
posterior_length <- 2000L
posterior_ids <- round(seq(1, posterior_length, length.out = samples))
    
out_ids <- lapply(ids, function(id_i) {
  this_dat <- dplyr::filter(gpdd, main_id == id_i)
  message(paste("main_id:", id_i))
  # x <- readRDS(paste0("gomp-base-normal/sm-", id_i, ".rds"))
  x <- readRDS(paste0("../scratch/gomp-base/sm-", id_i, ".rds"))
  x <- rstan::extract(x)

  out <- lapply(posterior_ids, function(i) {
    out_gomp <- sim_gomp_extinction(abundance_ts = this_dat$population_untransformed, 
      lambda = x$lambda[i], b = x$b[i], N = 10L, df = x$nu[i], 
      thresh = 3, sigma = x$sigma[i], id = id_i, bootstrap_residuals = FALSE, model = "heavy")
    out_gomp$posterior_sample <- i
    out_gomp
  })
  out_df <- do.call("rbind", out)
  out_df
})
out_df2 <- do.call("rbind", out_ids)
out_df2 <- mutate(out_df2, projection = ifelse(generation > 0, TRUE, FALSE))

 # out_df2 <- filter(out_df2, exp(dat) < 10e5)
library(ggplot2)
# p <- ggplot(out_df2, 
#   aes(generation, exp(dat), colour = projection, group = posterior_sample)) + 
#   geom_line(alpha = 0.2) + 
#   xlim(-30, 10) +
#   geom_vline(xintercept = 0, lty = 2) + 
#   geom_hline(yintercept = 10, lty = 2) + 
#   theme_bw() + ylab("Abundance") + xlab("Year") + 
#   scale_y_log10(breaks = scales::trans_breaks("log10", function(x) 10^x),
#     labels = scales::trans_format("log10", scales::math_format(10^.x)),
#     limits = c(1, 1e5)) + 
#   annotation_logticks(sides = "l") + 
#   guides(colour = guide_legend(override.aes= list(alpha = 1))) +
#   facet_wrap(~main_id) 
# print(p)
# ggsave("heavy-tailed-projections.png", width = 10, height = 6)
cvar <- function(x) mean(x[x<=quantile(x, probs = 0.01)])
group_by(out_df2, main_id) %>% summarise(cvar_ = cvar(exp(dat)))

out_df3 <- out_df2 %>% group_by(main_id, generation, projection) %>%
  summarise(
    min_dat = min(dat, na.rm = TRUE),
    q99 = quantile(dat, probs = 0.01, na.rm = TRUE)
    )

p <- ggplot(out_df3, 
  aes(generation, exp(min_dat), colour = projection)) + 
  geom_line() + 
  geom_line(aes(y = exp(q99)), lty = 2) + 
  geom_vline(xintercept = 0, lty = 2) + 
  geom_hline(yintercept = 10, lty = 2) + 
  xlim(-30, 10) +
  theme_bw() + ylab("Abundance") + xlab("Year") + 
  scale_y_log10(breaks = scales::trans_breaks("log10", function(x) 10^x),
    labels = scales::trans_format("log10", scales::math_format(10^.x)),
    limits = c(1, 1e4)) + 
  annotation_logticks(sides = "l") + 
  guides(colour = guide_legend(override.aes= list(alpha = 1))) +
  facet_wrap(~main_id) 
p

```

<!--#############################-->


<!--
```{r}
gpdd <- readRDS("gpdd-clean.rds") # to get the last abundance
dat <- readRDS("gomp-base-mean-sd.rds") # means and sds instead of medians
x <- gomp_hat_base[1,]
y1 <- filter(gpdd, main_id == x$main_id) %>% 
  filter(sample_year == max(sample_year))
y1 <- y1$population_untransformed
sim_gomp_extinction(
  y1 = y1,
  lambda = 
```
-->


Let's figure out what a good threshold for 10% of longterm mean is with our parameter values. We'll run a really long time series and get the mean abundance at nu = 3:

```{r}
expectation <- sim_gomp_extinction(N = 1e6, df = 3)
expect_abund <- exp(expectation$mean_log_abund)
expect_abund
thresh <- 0.1
```

Set up parallel processing:

```{r}
library("doParallel")
library("foreach")
registerDoParallel(cores = 4)
```

And run the simulation for 2000 repetitions across nu values of 2, 3, and 1e6 (i.e. normal):

```{r}
set.seed(123)
reps <- 2000
out <- list(length = 3)
out[[1]] <- plyr::ldply(seq_len(reps), function(i)
  sim_gomp_extinction(df = 2, thresh = expect_abund*thresh), .parallel = TRUE)
out[[2]] <- plyr::ldply(seq_len(reps), function(i)
  sim_gomp_extinction(df = 3, thresh = expect_abund*thresh), .parallel = TRUE)
out[[3]] <- plyr::ldply(seq_len(reps), function(i)
  sim_gomp_extinction(df = 1e6, thresh = expect_abund*thresh), .parallel = TRUE)
out <- do.call("rbind", out)
```

Plot the output:

```{r}
library("ggplot2")
ggplot(out, aes(as.factor(df), n_ext/N_burned)) + geom_boxplot() +
  ylab("Probability of abundance falling below 10% of longterm mean") +
  xlab(expression(nu~(t-distribution~degrees~of~freedom)))
```

Now we'll get some time series to plot as examples:

```{r}
set.seed(111)
out_plot <- list(length = 3)
reps <- 3
out_plot[[1]] <- plyr::ldply(seq_len(reps), function(i)
  sim_gomp_extinction(N = 200, df = 2, thresh = expect_abund*thresh, return_dat = TRUE, id = i))
out_plot[[2]] <- plyr::ldply(seq_len(reps), function(i)
  sim_gomp_extinction(N = 200, df = 3, thresh = expect_abund*thresh, return_dat = TRUE, id = i))
out_plot[[3]] <- plyr::ldply(seq_len(reps), function(i)
  sim_gomp_extinction(N = 200, df = 1e6, thresh = expect_abund*thresh, return_dat = TRUE, id = i))
out_plot <- do.call("rbind", out_plot)
```

Columns are repetitions. Rows are different degrees of heavy tails (nu values):

```{r}
ggplot(out_plot, aes(generation, dat)) + geom_point() + 
  geom_line() + facet_grid(df~id) +
  geom_hline(yintercept = log(expect_abund * thresh)) + 
  ylab("Scaled abundance") + xlab("Generation")

ggplot(out_plot, aes(generation, exp(dat))) + geom_point() + 
  geom_line() + facet_grid(df~id) +
  geom_hline(yintercept = expect_abund * thresh) + 
  ylab("log(Scaled abundance)") + xlab("Generation")
```
